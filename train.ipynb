{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: lightgbm in c:\\users\\sally\\appdata\\roaming\\python\\python312\\site-packages (4.6.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from lightgbm) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from lightgbm) (1.13.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sally\\AppData\\Local\\Temp\\ipykernel_23968\\241828049.py:13: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  train_df[col] = pd.to_numeric(train_df[col], errors=\"ignore\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "train_df = pd.read_csv(\"TrainOnMe.csv\")\n",
    "\n",
    "if \"Unnamed: 0\" in train_df.columns:\n",
    "    train_df.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "\n",
    "train_df.replace({\"Boom!\": np.nan, \"F\": np.nan}, inplace=True)\n",
    "\n",
    "for col in train_df.columns:\n",
    "    train_df[col] = pd.to_numeric(train_df[col], errors=\"ignore\")\n",
    "\n",
    "num_cols = train_df.select_dtypes(include=[\"number\"]).columns\n",
    "cat_cols = train_df.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "train_df[cat_cols] = train_df[cat_cols].astype(str)\n",
    "\n",
    "num_imputer = SimpleImputer(strategy=\"mean\")\n",
    "cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "train_df[num_cols] = num_imputer.fit_transform(train_df[num_cols])\n",
    "train_df[cat_cols] = cat_imputer.fit_transform(train_df[cat_cols])\n",
    "\n",
    "train_df.to_csv(\"TrainOnMe_Cleaned.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sally\\AppData\\Local\\Temp\\ipykernel_23968\\1601411045.py:13: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  eval_df[col] = pd.to_numeric(eval_df[col], errors=\"ignore\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "eval_df = pd.read_csv(\"EvaluateOnMe.csv\")\n",
    "\n",
    "if \"Unnamed: 0\" in eval_df.columns:\n",
    "    eval_df.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "\n",
    "eval_df.replace({\"?\": np.nan, \"F\": np.nan, \"Boom!\": np.nan}, inplace=True)\n",
    "\n",
    "for col in eval_df.columns:\n",
    "    eval_df[col] = pd.to_numeric(eval_df[col], errors=\"ignore\")\n",
    "\n",
    "num_cols = eval_df.select_dtypes(include=[\"number\"]).columns\n",
    "cat_cols = eval_df.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "num_imputer = SimpleImputer(strategy=\"mean\")\n",
    "cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "eval_df[num_cols] = num_imputer.fit_transform(eval_df[num_cols])\n",
    "eval_df[cat_cols] = cat_imputer.fit_transform(eval_df[cat_cols])\n",
    "\n",
    "def cap_outliers(df, column):\n",
    "    Q1, Q3 = df[column].quantile([0.25, 0.75])\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound, upper_bound = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
    "    df[column] = np.clip(df[column], lower_bound, upper_bound)\n",
    "\n",
    "for col in num_cols:\n",
    "    cap_outliers(eval_df, col)\n",
    "\n",
    "eval_df.to_csv(\"EvaluateOnMe_Cleaned.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer, OrdinalEncoder, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df = pd.read_csv(\"TrainOnMe_Cleaned.csv\")\n",
    "\n",
    "ordinal_cols = [\"x5\"]\n",
    "ordinal_encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "train_df[ordinal_cols] = ordinal_encoder.fit_transform(train_df[ordinal_cols])\n",
    "joblib.dump(ordinal_encoder, \"ordinal_encoder.pkl\")\n",
    "\n",
    "if \"x7\" in train_df.columns:\n",
    "    train_df = pd.get_dummies(train_df, columns=[\"x7\"], drop_first=True)\n",
    "\n",
    "num_cols = train_df.select_dtypes(include=[\"number\"]).columns\n",
    "\n",
    "num_imputer = SimpleImputer(strategy=\"mean\")\n",
    "train_df[num_cols] = num_imputer.fit_transform(train_df[num_cols])\n",
    "joblib.dump(num_imputer, \"num_imputer.pkl\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_df[num_cols] = scaler.fit_transform(train_df[num_cols])\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "\n",
    "power_transformer = PowerTransformer()\n",
    "train_df[num_cols] = power_transformer.fit_transform(train_df[num_cols])\n",
    "joblib.dump(power_transformer, \"power_transformer.pkl\")\n",
    "\n",
    "train_df.to_csv(\"TrainOnMe_FeatureEngineered.csv\", index=False)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "train_df[\"y\"] = label_encoder.fit_transform(train_df[\"y\"])\n",
    "joblib.dump(label_encoder, \"label_encoder.pkl\")\n",
    "\n",
    "train_df.to_csv(\"TrainOnMe_Encoded.csv\", index=False)\n",
    "\n",
    "X = train_df.drop(\"y\", axis=1)\n",
    "y = train_df[\"y\"]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "X_train.to_csv(\"X_train.csv\", index=False)\n",
    "X_val.to_csv(\"X_val.csv\", index=False)\n",
    "y_train.to_csv(\"y_train.csv\", index=False)\n",
    "y_val.to_csv(\"y_val.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer, OrdinalEncoder\n",
    "\n",
    "eval_df = pd.read_csv(\"EvaluateOnMe_Cleaned.csv\")\n",
    "\n",
    "num_imputer = joblib.load(\"num_imputer.pkl\")\n",
    "ordinal_encoder = joblib.load(\"ordinal_encoder.pkl\")\n",
    "scaler = joblib.load(\"scaler.pkl\")\n",
    "power_transformer = joblib.load(\"power_transformer.pkl\")\n",
    "\n",
    "num_cols = eval_df.select_dtypes(include=[\"number\"]).columns\n",
    "ordinal_cols = [\"x5\"]  \n",
    "\n",
    "eval_df[num_cols] = num_imputer.transform(eval_df[num_cols])\n",
    "\n",
    "eval_df[ordinal_cols] = ordinal_encoder.transform(eval_df[ordinal_cols])\n",
    "\n",
    "if \"x7\" in eval_df.columns:\n",
    "    eval_df = pd.get_dummies(eval_df, columns=[\"x7\"], drop_first=True)\n",
    "\n",
    "X_train = pd.read_csv(\"X_train.csv\")\n",
    "missing_cols = set(X_train.columns) - set(eval_df.columns)\n",
    "for col in missing_cols:\n",
    "    eval_df[col] = 0\n",
    "eval_df = eval_df[X_train.columns]\n",
    "\n",
    "eval_df[num_cols] = scaler.transform(eval_df[num_cols])\n",
    "eval_df[num_cols] = power_transformer.transform(eval_df[num_cols])\n",
    "\n",
    "eval_df.to_csv(\"EvaluateOnMe_Preprocessed.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Test Split: 0.1, K-Fold: 2, Accuracy: 0.6813\n",
      "Train-Test Split: 0.1, K-Fold: 3, Accuracy: 0.6853\n",
      "Train-Test Split: 0.1, K-Fold: 4, Accuracy: 0.6871\n",
      "Train-Test Split: 0.2, K-Fold: 2, Accuracy: 0.6687\n",
      "Train-Test Split: 0.2, K-Fold: 3, Accuracy: 0.6728\n",
      "Train-Test Split: 0.2, K-Fold: 4, Accuracy: 0.6713\n",
      "Train-Test Split: 0.3, K-Fold: 2, Accuracy: 0.6780\n",
      "Train-Test Split: 0.3, K-Fold: 3, Accuracy: 0.6871\n",
      "Train-Test Split: 0.3, K-Fold: 4, Accuracy: 0.6914\n",
      "Train-Test Split: 0.4, K-Fold: 2, Accuracy: 0.6737\n",
      "Train-Test Split: 0.4, K-Fold: 3, Accuracy: 0.6817\n",
      "Train-Test Split: 0.4, K-Fold: 4, Accuracy: 0.6787\n",
      "Best Test Split: 30.0%\n",
      "Best Model Trained with Accuracy = 0.6914\n",
      "Final best model has been trained and saved as 'final_model.pkl'.\n",
      "Best test dataset has been saved as 'X_test_best.csv' and 'y_test_best.csv'.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split, KFold, cross_val_score\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load Preprocessed Training Data\n",
    "train_df = pd.read_csv(\"TrainOnMe_Encoded.csv\")\n",
    "\n",
    "# Feature Engineering: Create an interaction term\n",
    "X = train_df.drop(\"y\", axis=1)\n",
    "y = train_df[\"y\"]\n",
    "\n",
    "class_counts = y.value_counts()\n",
    "scale_pos_weight = class_counts[0] / class_counts[1]  # Adjust if your minority class is different\n",
    "\n",
    "# Define Multiple Train-Test Split Percentages\n",
    "test_sizes = [0.1, 0.2, 0.3, 0.4]  # 90-10, 80-20, ..., 40-60\n",
    "k_folds = [ 2, 3, 4]  # You can modify this array to change K-Fold values\n",
    "\n",
    "best_split = None\n",
    "best_accuracy = 0\n",
    "best_model = None\n",
    "\n",
    "# Define Hyperparameter Grid\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 250, 500, 700, 1000],  \n",
    "    \"max_depth\": [None, 10, 20, 30, 40, 50],  \n",
    "    \"min_samples_split\": [2, 5, 10, 15],  \n",
    "    \"min_samples_leaf\": [1, 2, 4, 8],  \n",
    "    \"max_features\": [\"sqrt\", \"log2\", None]  \n",
    "}\n",
    "\n",
    "# Step 1: Loop Over Different Train-Test Splits\n",
    "for test_size in test_sizes:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, stratify=y, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Initialize RandomForestClassifier\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    \n",
    "    # Perform Randomized Search (Hyperparameter Tuning)\n",
    "    random_search = RandomizedSearchCV(\n",
    "        rf, param_distributions=param_grid, \n",
    "        n_iter=50,  \n",
    "        cv=5,  \n",
    "        scoring=\"accuracy\",\n",
    "        n_jobs=-1,  \n",
    "        verbose=0,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train the model with hyperparameter tuning\n",
    "    random_search.fit(X_train, y_train)\n",
    "    best_model_for_split = random_search.best_estimator_\n",
    "    \n",
    "    # Step 2: Perform K-Fold Cross-Validation\n",
    "    for fold in k_folds:\n",
    "        kf = KFold(n_splits=fold, shuffle=True, random_state=42)\n",
    "        scores = cross_val_score(best_model_for_split, X_train, y_train, cv=kf, scoring='accuracy')\n",
    "        accuracy = np.mean(scores)\n",
    "        print(f\"Train-Test Split: {test_size}, K-Fold: {fold}, Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Track Best Model and Split\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_model = best_model_for_split\n",
    "        best_split = (X_train, X_test, y_train, y_test, test_size)\n",
    "\n",
    "# Step 3: Train on the Best Split\n",
    "X_train_best, X_test_best, y_train_best, y_test_best, best_test_size = best_split\n",
    "best_model.fit(X_train_best, y_train_best)\n",
    "\n",
    "# Step 4: Save the Best Model\n",
    "joblib.dump(best_model, \"final_model.pkl\")\n",
    "\n",
    "# Step 5: Save Best Train-Test Split Data\n",
    "pd.DataFrame(X_test_best).to_csv(\"X_test_best.csv\", index=False)\n",
    "pd.DataFrame(y_test_best).to_csv(\"y_test_best.csv\", index=False)\n",
    "\n",
    "# Display Best Model Results\n",
    "print(f\"Best Test Split: {best_test_size:.1%}\")\n",
    "print(f\"Best Model Trained with Accuracy = {best_accuracy:.4f}\")\n",
    "print(f\"Final best model has been trained and saved as 'final_model.pkl'.\")\n",
    "print(f\"Best test dataset has been saved as 'X_test_best.csv' and 'y_test_best.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Predictions have been generated and saved in 'Label.txt'.\n",
      "Total Predictions: 10000\n",
      "First 5 Predictions:\n",
      "['Antrophic' 'OpenAI' 'Antrophic' 'OpenAI' 'Mistral']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "best_model = joblib.load(\"final_model.pkl\")\n",
    "eval_df = pd.read_csv(\"EvaluateOnMe_Preprocessed.csv\")\n",
    "predictions = best_model.predict(eval_df)\n",
    "label_encoder = joblib.load(\"label_encoder.pkl\")  # Load the saved LabelEncoder\n",
    "predictions_labels = label_encoder.inverse_transform(predictions)  # Correct label mapping\n",
    "with open(\"Label.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for label in predictions_labels:\n",
    "        f.write(label + \"\\n\")\n",
    "print(\"\\n Predictions have been generated and saved in 'Label.txt'.\")\n",
    "print(f\"Total Predictions: {len(predictions_labels)}\")\n",
    "print(\"First 5 Predictions:\")\n",
    "print(predictions_labels[:5])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
